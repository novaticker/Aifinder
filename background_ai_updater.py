import requests, json, os, re
from bs4 import BeautifulSoup
from datetime import datetime
import pytz

NEWS_FILE = "positive_news.json"
KST = pytz.timezone("Asia/Seoul")

def get_market_phase():
    now = datetime.now(KST)
    hour, minute = now.hour, now.minute
    total_minutes = hour * 60 + minute

    if 540 <= total_minutes < 1010:  # 09:00 ~ 16:50
        return "day"
    elif 1020 <= total_minutes <= 1350:  # 17:00 ~ 22:30
        return "pre"
    elif total_minutes > 1350 or total_minutes < 300:  # 22:30 ~ 05:00
        return "normal"
    else:  # 05:00 ~ 08:50
        return "after"

def true_ai_summarize(text):
    text_lower = text.lower()
    symbol_match = re.search(r"\(([A-Z]+)\)", text)
    symbol = symbol_match.group(1) if symbol_match else ""
    company = symbol if symbol else "Ìï¥Îãπ Í∏∞ÏóÖ"

    phrases = {
        "ÏûÑÏÉÅ": ["fda", "approval", "phase", "clinical", "trial"],
        "Ïù∏ÏàòÌï©Î≥ë": ["merger", "acquisition", "buyout"],
        "Ìà¨Ïûê": ["investment", "funding", "raise", "partnership"],
        "Ï†úÌíàÏ∂úÏãú": ["launch", "release", "expansion", "product", "platform"],
        "Ïã§Ï†Å": ["earnings", "revenue", "record", "profit", "financial"]
    }

    reasons = {
        "ÏûÑÏÉÅ": "Í∏çÏ†ïÏ†ÅÏù∏ ÏûÑÏÉÅ ÏÜåÏãùÏúºÎ°ú",
        "Ïù∏ÏàòÌï©Î≥ë": "M&A Î∞úÌëúÎ°ú",
        "Ìà¨Ïûê": "Ìà¨ÏûêÏú†Ïπò ÏÜåÏãùÏúºÎ°ú",
        "Ï†úÌíàÏ∂úÏãú": "Ï†úÌíà/ÏÑúÎπÑÏä§ Ï∂úÏãúÎ°ú",
        "Ïã§Ï†Å": "ÏñëÌò∏Ìïú Ïã§Ï†Å Î∞úÌëúÎ°ú"
    }

    for key, keywords in phrases.items():
        if any(k in text_lower for k in keywords):
            return f"{company}Îäî {reasons[key]} ÏãúÏû•Ïùò Ï£ºÎ™©ÏùÑ Î∞õÍ≥† ÏûàÏäµÎãàÎã§."

    return f"{company}Ïùò Í∏çÏ†ïÏ†ÅÏù∏ ÏÜåÏãùÏù¥ ÏãúÏû• Î∞òÏùëÏùÑ Ïù¥ÎÅåÍ≥† ÏûàÏäµÎãàÎã§."

def clean_symbol(text):
    m = re.search(r"\(([A-Z]+)\)", text)
    return m.group(1) if m else ""

def fetch_gainers_from_yahoo():
    try:
        url = "https://finance.yahoo.com/gainers"
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        gainers = []
        now = datetime.now(KST).strftime("%H:%M")

        for row in soup.select("table tbody tr"):
            cols = row.find_all("td")
            if len(cols) >= 6:
                symbol = cols[0].text.strip()
                price = cols[2].text.strip()
                percent = cols[4].text.strip()
                gainers.append({
                    "symbol": symbol,
                    "price": price,
                    "percent": percent,
                    "time": now,
                    "phase": get_market_phase()
                })
        return gainers
    except Exception as e:
        print(f"‚ùå Yahoo Gainers Error: {e}")
        return []

def fetch_news_from_prnews():
    news_list = []
    try:
        url = "https://www.prnewswire.com/news-releases/news-releases-list/"
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")

        for item in soup.select(".newsRelease"):
            title_tag = item.select_one(".newsTitle")
            if not title_tag:
                continue
            title = title_tag.text.strip()
            link_tag = title_tag.get("href")
            if not link_tag:
                continue
            symbol = clean_symbol(title)
            if not symbol:
                continue

            link = "https://www.prnewswire.com" + link_tag
            summary = true_ai_summarize(title)
            now = datetime.now(KST)
            news_item = {
                "title": title,
                "link": link,
                "summary": summary,
                "symbol": symbol,
                "time": now.strftime("%H:%M"),
                "timestamp": now.timestamp(),
                "date": now.strftime("%Y-%m-%d"),
                "source": "PRNewswire"
            }
            news_list.append(news_item)
    except Exception as e:
        print(f"‚ùå PRNews fetch error: {e}")
    return news_list

def save_data(news, gainers):
    today = datetime.now(KST).strftime("%Y-%m-%d")
    if os.path.exists(NEWS_FILE):
        with open(NEWS_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
    else:
        data = {}

    if today not in data:
        data[today] = {"news": [], "gainers": [], "signals": []}

    existing_titles = {n["title"] for n in data[today]["news"]}
    new_news = [n for n in news if n["title"] not in existing_titles]

    data[today]["news"].extend(new_news)
    data[today]["gainers"] = gainers

    with open(NEWS_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def update_news():
    print("ü§ñ AI ÌÉêÏÉâÍ∏∞ ÏàòÏßë ÏãúÏûë")
    gainers = fetch_gainers_from_yahoo()
    news = fetch_news_from_prnews()
    save_data(news, gainers)
    print("‚úÖ ÏàòÏßë ÏôÑÎ£å")

if __name__ == "__main__":
    update_news()
